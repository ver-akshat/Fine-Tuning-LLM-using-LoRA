{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Fine tuning Microsoft - Phi2 model on custom dataset\n* Phi2 is smaller model compared to other models\n* Using QLORA technique to do this\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q -U \"torch==2.1.2\" tensorboard\n!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:56:10.521219Z","iopub.execute_input":"2024-03-30T12:56:10.522196Z","iopub.status.idle":"2024-03-30T12:56:52.871547Z","shell.execute_reply.started":"2024-03-30T12:56:10.522158Z","shell.execute_reply":"2024-03-30T12:56:52.870555Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ns3fs 2024.3.0 requires fsspec==2024.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:57:25.538896Z","iopub.execute_input":"2024-03-30T12:57:25.539721Z","iopub.status.idle":"2024-03-30T12:58:22.390466Z","shell.execute_reply.started":"2024-03-30T12:57:25.539680Z","shell.execute_reply":"2024-03-30T12:58:22.389253Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# disbale WanB\nimport os\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:58:24.911808Z","iopub.execute_input":"2024-03-30T12:58:24.912683Z","iopub.status.idle":"2024-03-30T12:58:24.917059Z","shell.execute_reply.started":"2024-03-30T12:58:24.912631Z","shell.execute_reply":"2024-03-30T12:58:24.916195Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Imports\nfrom datasets import load_dataset\nfrom transformers import (AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,AutoTokenizer,\nTrainingArguments,Trainer,GenerationConfig,HfArgumentParser)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport transformers\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:25:20.504597Z","iopub.execute_input":"2024-03-30T13:25:20.505497Z","iopub.status.idle":"2024-03-30T13:25:34.273458Z","shell.execute_reply.started":"2024-03-30T13:25:20.505467Z","shell.execute_reply":"2024-03-30T13:25:34.272516Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  n\n"},{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading dataset from huggingface\nhuggingface_dataset_name=\"neil-code/dialogsum-test\"\ndataset=load_dataset(huggingface_dataset_name)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:59:15.460955Z","iopub.execute_input":"2024-03-30T12:59:15.461299Z","iopub.status.idle":"2024-03-30T12:59:24.880101Z","shell.execute_reply.started":"2024-03-30T12:59:15.461275Z","shell.execute_reply":"2024-03-30T12:59:24.879185Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b0e28b92eb42008307ef253bbbefa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebae12a5005e4c538cdca240a18c9a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66d31b3012147bab446988eb85d72d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c5810263dc4f40aecddf92657eba55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e0a2b4e83f483a829bdf23eaa083f1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bff0bce4c0f4f6099d5c80b0fd1e834"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4915e5d9a42343158839982212a3717b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# looking at dataset\ndataset['train'][2]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:59:28.776871Z","iopub.execute_input":"2024-03-30T12:59:28.777229Z","iopub.status.idle":"2024-03-30T12:59:28.783779Z","shell.execute_reply.started":"2024-03-30T12:59:28.777202Z","shell.execute_reply":"2024-03-30T12:59:28.782925Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_2',\n 'dialogue': \"#Person1#: Excuse me, did you see a set of keys?\\n#Person2#: What kind of keys?\\n#Person1#: Five keys and a small foot ornament.\\n#Person2#: What a shame! I didn't see them.\\n#Person1#: Well, can you help me look for it? That's my first time here.\\n#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\\n#Person1#: It's very kind of you.\\n#Person2#: It's not a big deal.Hey, I found them.\\n#Person1#: Oh, thank God! I don't know how to thank you, guys.\\n#Person2#: You're welcome.\",\n 'summary': \"#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\",\n 'topic': 'find keys'}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Creating bnb config- we need configuration class to specify which kind of quantization we need, using bitsandbytes(bnb) for making this configuration so that LLM can be loaded in 4 bits so that memory utilization is less and model can be loaded on small devices","metadata":{}},{"cell_type":"markdown","source":"#### Load Base Model Phi2 in 4 bit Quantization using bnb config","metadata":{}},{"cell_type":"code","source":"model_name = 'microsoft/phi-2'\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:59:36.161887Z","iopub.execute_input":"2024-03-30T12:59:36.162280Z","iopub.status.idle":"2024-03-30T13:01:00.084720Z","shell.execute_reply.started":"2024-03-30T12:59:36.162250Z","shell.execute_reply":"2024-03-30T13:01:00.083955Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bdd0ff086b24301a1b5ce79c616c06c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bcae85bd72043249b07ec2c79bfc9f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e5674645c247488aa2a035c286de11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf590983c52485aa84daee55aa8a699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa489373d48241e1ab8db056081b7ecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6d0268111d42fd981259b64e55c86d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.26.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight']\n- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.1.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a39538dc36c4c4cb1d36528a59564b9"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Setting tokenizer - using left padding to save memory","metadata":{}},{"cell_type":"code","source":"# Setting tokenizer\ntokenizer=AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,\n                                        padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token=tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:00.086135Z","iopub.execute_input":"2024-03-30T13:01:00.086417Z","iopub.status.idle":"2024-03-30T13:01:04.427024Z","shell.execute_reply.started":"2024-03-30T13:01:00.086393Z","shell.execute_reply":"2024-03-30T13:01:04.426096Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09de52944e304ce2b987c44b7e2329d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab04b318201d46d5b922daf1e5c91bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e696f930fb47eca7b8333118699358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124e286d9c5f452997f534ed0b476d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a691c78bee6546348d4d28dbc01a8201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d33878f2f642d3b9c93ac54de8a4c0"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Creating a funtion to track GPU utilization at regular intervals","metadata":{}},{"cell_type":"code","source":"from pynvml import *\ndef print_gpu_utilization():\n    nvmlInit()\n    handle=nvmlDeviceGetHandleByIndex(0)\n    info=nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:08.866720Z","iopub.execute_input":"2024-03-30T13:01:08.867413Z","iopub.status.idle":"2024-03-30T13:01:08.872838Z","shell.execute_reply.started":"2024-03-30T13:01:08.867380Z","shell.execute_reply":"2024-03-30T13:01:08.871486Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:10.066440Z","iopub.execute_input":"2024-03-30T13:01:10.067183Z","iopub.status.idle":"2024-03-30T13:01:10.072057Z","shell.execute_reply.started":"2024-03-30T13:01:10.067150Z","shell.execute_reply":"2024-03-30T13:01:10.071108Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"GPU memory occupied: 2229 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Defining evaluation tokenizer-","metadata":{}},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:13.174567Z","iopub.execute_input":"2024-03-30T13:01:13.175408Z","iopub.status.idle":"2024-03-30T13:01:13.481672Z","shell.execute_reply.started":"2024-03-30T13:01:13.175374Z","shell.execute_reply":"2024-03-30T13:01:13.480722Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Testing model with zero shot inferencing","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed=42\nset_seed(seed)\nindex=10\nprompt=dataset['test'][index]['dialogue']\nsummary=dataset['train'][index]['summary']\nformatted_prompt=f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres=gen(model,formatted_prompt,100,)\noutput=res[0].split('Output:\\n')[1]\nsep_line='*'*100\nprint(sep_line)\nprint(f'Input Prompt:\\n{formatted_prompt}\\n')\nprint(sep_line)\nprint(f'Baseline Human summary:\\n{summary}\\n')\nprint(sep_line)\nprint(f'Model Generated Zero Shot\"\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:21.603642Z","iopub.execute_input":"2024-03-30T13:01:21.604375Z","iopub.status.idle":"2024-03-30T13:01:29.510844Z","shell.execute_reply.started":"2024-03-30T13:01:21.604344Z","shell.execute_reply":"2024-03-30T13:01:29.509918Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"****************************************************************************************************\nInput Prompt:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n\n****************************************************************************************************\nBaseline Human summary:\n#Person1# asks #Person2# to do a favor. #Person2# agrees and helps buy a small bag of sugar, six oranges, and a half-gallon of milk.\n\n****************************************************************************************************\nModel Generated Zero Shot\"\n some of the two is some of the two is some of some of the two and some of the two and some of the two and some of some of the two is some of some of the two under the two and some of the two and some of some of some of some of the two and some of some of some of some of some of some of some of some of some of some of the two and some of some of some of some of the two and some of some of some of\nCPU times: user 7.24 s, sys: 235 ms, total: 7.47 s\nWall time: 7.9 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Preprocess dataset","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output'), concatenate them\n    \"\"\"\n    INTRO_BLURB=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY=\"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY=\"### Output:\"\n    END_KEY=\"### End\"\n    blurb=f\"\\n{INTRO_BLURB}\"\n    instruction=f\"{INSTRUCTION_KEY}\"\n    input_context=f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response=f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end=f\"{END_KEY}\"\n    parts=[part for part in [blurb,instruction,input_context,response,end] if part]\n    formatted_prompt=\"\\n\\n\".join(parts)\n    sample[\"text\"]=formatted_prompt\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:01:41.999184Z","iopub.execute_input":"2024-03-30T13:01:41.999945Z","iopub.status.idle":"2024-03-30T13:01:42.006318Z","shell.execute_reply.started":"2024-03-30T13:01:41.999902Z","shell.execute_reply":"2024-03-30T13:01:42.005467Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_max_length(model):\n    conf=model.config\n    max_length=None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length=getattr(model.config,length_setting,None)\n        if max_length:\n            print(f\"FOund max length:{max_length}\")\n            break\n    if not max_length:\n        max_length=1024\n        print(\"Using default max length\")\n    return max_length\n\ndef preprocess_batch(batch,tokenizer,max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(batch[\"text\"],max_length=max_length,truncation=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:02:10.194776Z","iopub.execute_input":"2024-03-30T13:02:10.195155Z","iopub.status.idle":"2024-03-30T13:02:10.202773Z","shell.execute_reply.started":"2024-03-30T13:02:10.195128Z","shell.execute_reply":"2024-03-30T13:02:10.201506Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Format and tokenize the dataset to make it training ready","metadata":{}},{"cell_type":"code","source":"from functools import partial\ndef preprocess_dataset(tokenizer:AutoTokenizer,max_length:int,seed,dataset):\n    print(\"Preprocessing Dataset\\n\")\n    dataset=dataset.map(create_prompt_formats)\n    preprocessing_function=partial(preprocess_batch,max_length=max_length,tokenizer=tokenizer)\n    dataset=dataset.map(preprocessing_function,batched=True,remove_columns=['id','topic','dialogue','summary'])\n    dataset=dataset.filter(lambda sample:len(sample[\"input_ids\"])<max_length)\n    dataset=dataset.shuffle(seed=seed)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:02:17.458610Z","iopub.execute_input":"2024-03-30T13:02:17.459039Z","iopub.status.idle":"2024-03-30T13:02:17.465386Z","shell.execute_reply.started":"2024-03-30T13:02:17.459008Z","shell.execute_reply":"2024-03-30T13:02:17.464422Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:02:20.165802Z","iopub.execute_input":"2024-03-30T13:02:20.166166Z","iopub.status.idle":"2024-03-30T13:02:20.171498Z","shell.execute_reply.started":"2024-03-30T13:02:20.166137Z","shell.execute_reply":"2024-03-30T13:02:20.170502Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"GPU memory occupied: 2579 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Finally preprocess dataset","metadata":{}},{"cell_type":"code","source":"max_length=get_max_length(model)\nprint(max_length)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:02:22.753689Z","iopub.execute_input":"2024-03-30T13:02:22.754058Z","iopub.status.idle":"2024-03-30T13:02:22.758802Z","shell.execute_reply.started":"2024-03-30T13:02:22.754031Z","shell.execute_reply":"2024-03-30T13:02:22.757885Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"FOund max length:2048\n2048\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset=preprocess_dataset(tokenizer,max_length,seed,dataset['train'])\neval_dataset=preprocess_dataset(tokenizer,max_length,seed,dataset['validation'])\nprint(\"Shapes of datasets\\n\")\nprint(f\"Training dataset:{train_dataset.shape}\")\nprint(f\"Validation dataset:{eval_dataset.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:02:25.445742Z","iopub.execute_input":"2024-03-30T13:02:25.446111Z","iopub.status.idle":"2024-03-30T13:02:38.643629Z","shell.execute_reply.started":"2024-03-30T13:02:25.446084Z","shell.execute_reply":"2024-03-30T13:02:38.642876Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Preprocessing Dataset\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3369d2876109414ba33a460ed0560d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1baff4460a041c69552ea18035b2331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfdfe71d56b0469b8ba3a1cc15648a5e"}},"metadata":{}},{"name":"stdout","text":"Preprocessing Dataset\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef72849108344a9da98f653b1bdfcf9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b87ec61fd8345f0857e5fb35692296e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb224f729b94465e812dacb19571f5b2"}},"metadata":{}},{"name":"stdout","text":"Shapes of datasets\n\nTraining dataset:(1999, 3)\nValidation dataset:(499, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:03:04.264004Z","iopub.execute_input":"2024-03-30T13:03:04.264377Z","iopub.status.idle":"2024-03-30T13:03:04.269285Z","shell.execute_reply.started":"2024-03-30T13:03:04.264347Z","shell.execute_reply":"2024-03-30T13:03:04.268292Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Setup PEFT and LORA model for fine tuning\n* PEFT is a form of instruction fine tuning more efficient than full fine tuning, it comes in2 variants - LoRA and QLoRA, LoRA is used here, it used lower resources.\n* Doing LoRA finetuning results in an unchanged original LLM & smaller LoRA adapter, during inference time LoRA adapter must be combined with original LLM","metadata":{}},{"cell_type":"markdown","source":"#### Creating a method to calculate number of trainable parameters","metadata":{}},{"cell_type":"code","source":"def trainable_model_parameters_count(model):\n    trainable_model_params=0\n    all_model_params=0\n    for _,param in model.named_parameters():\n        all_model_params+=param.numel()\n        if param.requires_grad:\n            trainable_model_params+=param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:11:46.902016Z","iopub.execute_input":"2024-03-30T13:11:46.902352Z","iopub.status.idle":"2024-03-30T13:11:46.907691Z","shell.execute_reply.started":"2024-03-30T13:11:46.902329Z","shell.execute_reply":"2024-03-30T13:11:46.906816Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(trainable_model_parameters_count(model))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:11:48.199181Z","iopub.execute_input":"2024-03-30T13:11:48.199868Z","iopub.status.idle":"2024-03-30T13:11:48.207011Z","shell.execute_reply.started":"2024-03-30T13:11:48.199833Z","shell.execute_reply":"2024-03-30T13:11:48.206080Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:12:06.073872Z","iopub.execute_input":"2024-03-30T13:12:06.074606Z","iopub.status.idle":"2024-03-30T13:12:06.081544Z","shell.execute_reply.started":"2024-03-30T13:12:06.074575Z","shell.execute_reply":"2024-03-30T13:12:06.080647Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (query_key_value): Linear4bit(in_features=2560, out_features=7680, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\nconfig=LoraConfig(r=32,lora_alpha=32,target_modules=[\n    'q_proj','k_proj','v_proj','dense'\n],bias='none',lora_dropout=0.07,task_type=\"CAUSAL_LM\")\n#Activating gradient checkpointing to reduce memory usage during fine-tuning\nmodel.gradient_checkpointing_enable()\n#Using the prepare_model_for_kbit_training method from PEFT\nmodel=prepare_model_for_kbit_training(model)\npeft_model=get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:17:34.669386Z","iopub.execute_input":"2024-03-30T13:17:34.669735Z","iopub.status.idle":"2024-03-30T13:17:34.795446Z","shell.execute_reply.started":"2024-03-30T13:17:34.669710Z","shell.execute_reply":"2024-03-30T13:17:34.794475Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### Now PEFT configuration is setup, find number of trainable parameters in this model","metadata":{}},{"cell_type":"code","source":"print(trainable_model_parameters_count(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:18:56.442726Z","iopub.execute_input":"2024-03-30T13:18:56.443375Z","iopub.status.idle":"2024-03-30T13:18:56.451579Z","shell.execute_reply.started":"2024-03-30T13:18:56.443343Z","shell.execute_reply":"2024-03-30T13:18:56.450591Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"trainable model parameters: 5242880\nall model parameters: 1526635520\npercentage of trainable model parameters: 0.34%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### We can see in PEFT number of trainable parameters is much less compared to original model","metadata":{}},{"cell_type":"code","source":"print(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:19:40.043150Z","iopub.execute_input":"2024-03-30T13:19:40.043510Z","iopub.status.idle":"2024-03-30T13:19:40.052750Z","shell.execute_reply.started":"2024-03-30T13:19:40.043483Z","shell.execute_reply":"2024-03-30T13:19:40.051825Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (query_key_value): Linear4bit(in_features=2560, out_features=7680, bias=True)\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.07, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n              (rotary_emb): PhiRotaryEmbedding()\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Define training arguments and setup trainer","metadata":{}},{"cell_type":"code","source":"output_dir='./peft-dialogue-summary-training/final-checkpoint'\npeft_training_args=TrainingArguments(output_dir=output_dir,warmup_steps=1,\n                        per_device_train_batch_size=1,gradient_accumulation_steps=5,\n                        max_steps=200,learning_rate=2e-4,optim=\"paged_adamw_8bit\",\n                        logging_steps=25,logging_dir=\"./logs\",save_strategy=\"steps\",\n                        save_steps=25,evaluation_strategy=\"steps\",eval_steps=25,\n                        do_eval=True,gradient_checkpointing=True,report_to=None,\n                                    overwrite_output_dir='True',group_by_length=True)\npeft_model.config.use_cache=False\npeft_trainer=transformers.Trainer(model=peft_model,train_dataset=train_dataset,\n                                 eval_dataset=eval_dataset,args=peft_training_args,\n            data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,mlm=False))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:36:44.558909Z","iopub.execute_input":"2024-03-30T13:36:44.559633Z","iopub.status.idle":"2024-03-30T13:36:44.573363Z","shell.execute_reply.started":"2024-03-30T13:36:44.559602Z","shell.execute_reply":"2024-03-30T13:36:44.572455Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_training_args.device","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:36:47.245425Z","iopub.execute_input":"2024-03-30T13:36:47.246222Z","iopub.status.idle":"2024-03-30T13:36:47.252569Z","shell.execute_reply.started":"2024-03-30T13:36:47.246188Z","shell.execute_reply":"2024-03-30T13:36:47.251465Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:36:48.245362Z","iopub.execute_input":"2024-03-30T13:36:48.246273Z","iopub.status.idle":"2024-03-30T13:36:48.251315Z","shell.execute_reply.started":"2024-03-30T13:36:48.246241Z","shell.execute_reply":"2024-03-30T13:36:48.250293Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"GPU memory occupied: 10533 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:36:51.087127Z","iopub.execute_input":"2024-03-30T13:36:51.087755Z","iopub.status.idle":"2024-03-30T14:20:15.926767Z","shell.execute_reply.started":"2024-03-30T13:36:51.087722Z","shell.execute_reply":"2024-03-30T14:20:15.925912Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 43:10, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.656100</td>\n      <td>3.294434</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.850300</td>\n      <td>3.303666</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.488400</td>\n      <td>3.147907</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.794800</td>\n      <td>3.244513</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>3.319400</td>\n      <td>3.124656</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.679900</td>\n      <td>3.206429</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>3.390600</td>\n      <td>3.099191</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.777800</td>\n      <td>3.096688</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory ./peft-dialogue-summary-training/final-checkpoint/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=3.1196464920043945, metrics={'train_runtime': 2603.2653, 'train_samples_per_second': 0.384, 'train_steps_per_second': 0.077, 'total_flos': 4535368716257280.0, 'train_loss': 3.1196464920043945, 'epoch': 0.5})"},"metadata":{}}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:20:39.301593Z","iopub.execute_input":"2024-03-30T14:20:39.301993Z","iopub.status.idle":"2024-03-30T14:20:39.307342Z","shell.execute_reply.started":"2024-03-30T14:20:39.301961Z","shell.execute_reply":"2024-03-30T14:20:39.306438Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"GPU memory occupied: 10543 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Evaluate the model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nbase_model_id=\"microsoft/phi-2\"\nbase_model=AutoModelForCausalLM.from_pretrained(base_model_id,device_map='auto',\n                                quantization_config=bnb_config,trust_remote_code=True,use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:21:32.240144Z","iopub.execute_input":"2024-03-30T14:21:32.240536Z","iopub.status.idle":"2024-03-30T14:21:37.711145Z","shell.execute_reply.started":"2024-03-30T14:21:32.240506Z","shell.execute_reply":"2024-03-30T14:21:37.710349Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ddccdd376ad4a088e3c10538e1fe7be"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d2ffbafd43428987ed0122668abaa8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2191cb273d6b443ba80ab6f60a6e99db"}},"metadata":{}}]},{"cell_type":"code","source":"eval_tokenizer=AutoTokenizer.from_pretrained(base_model_id,add_bos_token=True,trust_remote_code=True,use_fast=False)\neval_tokenizer.pad_token=eval_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:22:53.427803Z","iopub.execute_input":"2024-03-30T14:22:53.428200Z","iopub.status.idle":"2024-03-30T14:22:53.743630Z","shell.execute_reply.started":"2024-03-30T14:22:53.428172Z","shell.execute_reply":"2024-03-30T14:22:53.742698Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\nft_model=PeftModel.from_pretrained(base_model,\"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-200\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:24:17.059062Z","iopub.execute_input":"2024-03-30T14:24:17.059757Z","iopub.status.idle":"2024-03-30T14:24:17.559598Z","shell.execute_reply.started":"2024-03-30T14:24:17.059728Z","shell.execute_reply":"2024-03-30T14:24:17.558804Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\nindex=10\ndialogue=dataset['test'][index]['dialogue']\nsummary=dataset['test'][index]['summary']\nprompt=f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\npeft_model_res=gen(ft_model,prompt,100,)\npeft_model_output=peft_model_res[0].split('Output:\\n')[1]\nprefix,success,result=peft_model_output.partition('#End')\nsep_line=\"*\"*100\nprint(sep_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(sep_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(sep_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:25:50.020424Z","iopub.execute_input":"2024-03-30T14:25:50.020827Z","iopub.status.idle":"2024-03-30T14:25:56.522518Z","shell.execute_reply.started":"2024-03-30T14:25:50.020798Z","shell.execute_reply":"2024-03-30T14:25:56.521560Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"****************************************************************************************************\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n****************************************************************************************************\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n****************************************************************************************************\nPEFT MODEL:\nPerson1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n\nCPU times: user 6.44 s, sys: 53.3 ms, total: 6.49 s\nWall time: 6.5 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### The above evaluation is just qualitative without using any metric , now trying to evaluate model quantitatively using ROUGE metric\n* Doing this only for few samples only to save time(7 samples)","metadata":{}},{"cell_type":"code","source":"original_model=AutoModelForCausalLM.from_pretrained(base_model_id,device_map=\"auto\",\n                quantization_config=bnb_config,trust_remote_code=True,use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:29:32.671537Z","iopub.execute_input":"2024-03-30T14:29:32.672226Z","iopub.status.idle":"2024-03-30T14:29:37.515039Z","shell.execute_reply.started":"2024-03-30T14:29:32.672193Z","shell.execute_reply":"2024-03-30T14:29:37.514184Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38015f6b940a479096ceede57e4ceb05"}},"metadata":{}}]},{"cell_type":"code","source":"dialogues=dataset['test'][0:7]['dialogue']\ndataset_baseline_summaries=dataset['test'][0:7]['summary']\noriginal_model_summaries=[]\npeft_model_summaries=[]\nfor idx,dialogue in enumerate(dialogues):\n    baseline_text_output=dataset_baseline_summaries[idx]\n    prompt=f\"Instruct: Summarize the following conversation:\\n{dialogue}\\nOutput:\\n\"\n    original_model_res=gen(original_model,prompt,100,)\n    original_model_text_output=original_model_res[0].split('Output:\\n')[1]\n    peft_model_res=gen(ft_model,prompt,100,)\n    peft_model_output=peft_model_res[0].split('Output:\\n')[1]\n    peft_model_text_output,success,result=peft_model_output.partition('#End')\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\nzipped_summaries=list(zip(dataset_baseline_summaries,original_model_summaries,peft_model_summaries))\ndf=pd.DataFrame(zipped_summaries,columns=['dataset_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:44:36.941179Z","iopub.execute_input":"2024-03-30T14:44:36.941574Z","iopub.status.idle":"2024-03-30T14:46:33.798568Z","shell.execute_reply.started":"2024-03-30T14:44:36.941547Z","shell.execute_reply":"2024-03-30T14:46:33.797611Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                          dataset_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n\n                            original_model_summaries  \\\n0  Person 1: Ms. Dawson, I need you to take a dic...   \n1  Person 1: Ms. Dawson, I need you to take a dic...   \n2  Person 1: Ms. Dawson, I need you to take a dic...   \n3  Person1 and Person2 are discussing the traffic...   \n4  Person1 and Person2 are discussing the traffic...   \n5  Person1 and Person2 are discussing the traffic...   \n6  Kate: Masha and Hero are getting divorced afte...   \n\n                                peft_model_summaries  \n0  Person 1: Ms. Dawson, I need you to take a dic...  \n1  Person 1: Ms. Dawson, I need you to take a dic...  \n2  Person 1: Ms. Dawson, I need you to take a dic...  \n3  Person1 and Person2 are discussing the traffic...  \n4  Person1 and Person2 are discussing the traffic...  \n5  Person1 and Person2 are discussing the traffic...  \n6  Kate: Masha and Hero are getting divorced afte...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Kate: Masha and Hero are getting divorced afte...</td>\n      <td>Kate: Masha and Hero are getting divorced afte...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:47:02.243691Z","iopub.execute_input":"2024-03-30T14:47:02.244317Z","iopub.status.idle":"2024-03-30T14:47:16.833027Z","shell.execute_reply.started":"2024-03-30T14:47:02.244287Z","shell.execute_reply":"2024-03-30T14:47:16.831870Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=824b7dce9c9f9039267f8de38fef4dd8582a8c4487961ae1c32d728a51275113\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:54:36.352437Z","iopub.execute_input":"2024-03-30T14:54:36.352871Z","iopub.status.idle":"2024-03-30T14:54:48.989566Z","shell.execute_reply.started":"2024-03-30T14:54:36.352838Z","shell.execute_reply":"2024-03-30T14:54:48.988371Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.16.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\noriginal_model_results=rouge.compute(\n    predictions=original_model_summaries,\n    references=dataset_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\npeft_model_results=rouge.compute(\n    predictions=peft_model_summaries,\n    references=dataset_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:55:43.961813Z","iopub.execute_input":"2024-03-30T14:55:43.962225Z","iopub.status.idle":"2024-03-30T14:55:46.148500Z","shell.execute_reply.started":"2024-03-30T14:55:43.962197Z","shell.execute_reply":"2024-03-30T14:55:46.147406Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.27667979627279304, 'rouge2': 0.08875241694518803, 'rougeL': 0.19135388883743298, 'rougeLsum': 0.21150262085032032}\nPEFT MODEL:\n{'rouge1': 0.3040309821949655, 'rouge2': 0.10936087174375382, 'rougeL': 0.20278568805863828, 'rougeLsum': 0.2368705591597158}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\nimprovement=(np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:56:35.481919Z","iopub.execute_input":"2024-03-30T14:56:35.482585Z","iopub.status.idle":"2024-03-30T14:56:35.488556Z","shell.execute_reply.started":"2024-03-30T14:56:35.482545Z","shell.execute_reply":"2024-03-30T14:56:35.487467Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 2.74%\nrouge2: 2.06%\nrougeL: 1.14%\nrougeLsum: 2.54%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}